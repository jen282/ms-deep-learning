{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1720147789183
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\ms-deep-learning\\deep-learning\\notebooks\\deep-learning-basic\n",
            "c:\\Users\\USER\\ms-deep-learning\\deep-learning\\notebooks\n"
          ]
        }
      ],
      "source": [
        "import os, sys\n",
        "print(os.getcwd())\n",
        "current_dir = os.path.dirname(os.getcwd())\n",
        "print(current_dir)\n",
        "os.chdir(current_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from common.util import smooth_curve\n",
        "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
        "from common.trainer import Trainer\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss:2.3124552691552243\n",
            "=== epoch:1, train acc:0.12666666666666668, test acc:0.1256 ===\n",
            "train loss:2.308465887478421\n",
            "train loss:2.3293115882051567\n",
            "train loss:2.3044512862497024\n",
            "=== epoch:2, train acc:0.13, test acc:0.1296 ===\n",
            "train loss:2.3029313004202394\n",
            "train loss:2.294055543893945\n",
            "train loss:2.3085944936507796\n",
            "=== epoch:3, train acc:0.12333333333333334, test acc:0.1317 ===\n",
            "train loss:2.312169141448125\n",
            "train loss:2.2987459111205033\n",
            "train loss:2.293993040195498\n",
            "=== epoch:4, train acc:0.12666666666666668, test acc:0.1339 ===\n",
            "train loss:2.3091625865608036\n",
            "train loss:2.3088681247983844\n",
            "train loss:2.312364326014142\n",
            "=== epoch:5, train acc:0.13, test acc:0.1362 ===\n",
            "train loss:2.3039369099095186\n",
            "train loss:2.297192630461689\n",
            "train loss:2.305141771215489\n",
            "=== epoch:6, train acc:0.14333333333333334, test acc:0.1379 ===\n",
            "train loss:2.306320629996309\n",
            "train loss:2.3001735422267937\n",
            "train loss:2.302896893387333\n",
            "=== epoch:7, train acc:0.15333333333333332, test acc:0.1356 ===\n",
            "train loss:2.296711005347735\n",
            "train loss:2.302211015988034\n",
            "train loss:2.285644513488693\n",
            "=== epoch:8, train acc:0.15333333333333332, test acc:0.1376 ===\n",
            "train loss:2.3034410086093566\n",
            "train loss:2.297571874012485\n",
            "train loss:2.3009711668974275\n",
            "=== epoch:9, train acc:0.15, test acc:0.1407 ===\n",
            "train loss:2.2986169849166322\n",
            "train loss:2.2906005331011894\n",
            "train loss:2.2928391650274547\n",
            "=== epoch:10, train acc:0.15666666666666668, test acc:0.141 ===\n",
            "train loss:2.3000183142516537\n",
            "train loss:2.30081597700734\n",
            "train loss:2.289534852409474\n",
            "=== epoch:11, train acc:0.15333333333333332, test acc:0.1387 ===\n",
            "train loss:2.288420512322465\n",
            "train loss:2.285798378815043\n",
            "train loss:2.2921078142809175\n",
            "=== epoch:12, train acc:0.15, test acc:0.1389 ===\n",
            "train loss:2.314136525182025\n",
            "train loss:2.307951455191871\n",
            "train loss:2.298353497583327\n",
            "=== epoch:13, train acc:0.14333333333333334, test acc:0.1363 ===\n",
            "train loss:2.2944755805624117\n",
            "train loss:2.292944977649873\n",
            "train loss:2.2846453096830386\n",
            "=== epoch:14, train acc:0.16, test acc:0.1349 ===\n",
            "train loss:2.29306487851395\n",
            "train loss:2.2821312884238747\n",
            "train loss:2.3052609185177544\n",
            "=== epoch:15, train acc:0.16333333333333333, test acc:0.1372 ===\n",
            "train loss:2.2833673550014497\n",
            "train loss:2.2887100230803132\n",
            "train loss:2.291484726801955\n",
            "=== epoch:16, train acc:0.16333333333333333, test acc:0.1388 ===\n",
            "train loss:2.284193994414902\n",
            "train loss:2.299003032045012\n",
            "train loss:2.2840499508643135\n",
            "=== epoch:17, train acc:0.16333333333333333, test acc:0.1394 ===\n",
            "train loss:2.2912993457990494\n",
            "train loss:2.298535905797243\n",
            "train loss:2.280388709114792\n",
            "=== epoch:18, train acc:0.16333333333333333, test acc:0.138 ===\n",
            "train loss:2.2843596339626893\n",
            "train loss:2.2865274875021213\n",
            "train loss:2.2858276902014434\n",
            "=== epoch:19, train acc:0.16333333333333333, test acc:0.1375 ===\n",
            "train loss:2.2900955464716723\n",
            "train loss:2.2945910935643385\n",
            "train loss:2.2755909880852783\n",
            "=== epoch:20, train acc:0.16333333333333333, test acc:0.1376 ===\n",
            "train loss:2.289796850836055\n",
            "train loss:2.3033662202686624\n",
            "train loss:2.280158491054511\n",
            "=== epoch:21, train acc:0.16333333333333333, test acc:0.1384 ===\n",
            "train loss:2.277696843934132\n",
            "train loss:2.294296177054366\n",
            "train loss:2.2936428833954725\n",
            "=== epoch:22, train acc:0.16666666666666666, test acc:0.1386 ===\n",
            "train loss:2.274349427221963\n",
            "train loss:2.30153475495555\n",
            "train loss:2.2863847709991933\n",
            "=== epoch:23, train acc:0.17, test acc:0.1403 ===\n",
            "train loss:2.2874827686533705\n",
            "train loss:2.277370890787817\n",
            "train loss:2.281507674762224\n",
            "=== epoch:24, train acc:0.16666666666666666, test acc:0.1407 ===\n",
            "train loss:2.290916691378908\n",
            "train loss:2.2959651288428553\n",
            "train loss:2.2792857592148175\n",
            "=== epoch:25, train acc:0.17333333333333334, test acc:0.1428 ===\n",
            "train loss:2.2758451563518416\n",
            "train loss:2.289147915914874\n",
            "train loss:2.2840613640585317\n",
            "=== epoch:26, train acc:0.17333333333333334, test acc:0.1449 ===\n",
            "train loss:2.287297725288651\n",
            "train loss:2.2794025328602383\n",
            "train loss:2.2810336536739206\n",
            "=== epoch:27, train acc:0.17666666666666667, test acc:0.1453 ===\n",
            "train loss:2.2839913850990348\n",
            "train loss:2.2719085031117854\n",
            "train loss:2.2728452167276982\n",
            "=== epoch:28, train acc:0.19, test acc:0.1457 ===\n",
            "train loss:2.269866774435434\n",
            "train loss:2.2826923713589466\n",
            "train loss:2.279511951674517\n",
            "=== epoch:29, train acc:0.19333333333333333, test acc:0.147 ===\n",
            "train loss:2.2798862813192837\n",
            "train loss:2.275445479600189\n",
            "train loss:2.283210909843946\n",
            "=== epoch:30, train acc:0.2, test acc:0.1484 ===\n",
            "train loss:2.2780167042711943\n",
            "train loss:2.28053095651455\n",
            "train loss:2.2698650532549305\n",
            "=== epoch:31, train acc:0.20333333333333334, test acc:0.1507 ===\n",
            "train loss:2.289517688972119\n",
            "train loss:2.284273319622395\n",
            "train loss:2.267523651626364\n",
            "=== epoch:32, train acc:0.2, test acc:0.1523 ===\n",
            "train loss:2.2763357103090915\n",
            "train loss:2.2784283984549223\n",
            "train loss:2.2800262586098468\n",
            "=== epoch:33, train acc:0.20333333333333334, test acc:0.1541 ===\n",
            "train loss:2.260802618285984\n",
            "train loss:2.2728063069267157\n",
            "train loss:2.2685458880559746\n",
            "=== epoch:34, train acc:0.2, test acc:0.1575 ===\n",
            "train loss:2.2750795856334305\n",
            "train loss:2.2860242496673884\n",
            "train loss:2.2539927346616055\n",
            "=== epoch:35, train acc:0.19666666666666666, test acc:0.1588 ===\n",
            "train loss:2.2706036686036635\n",
            "train loss:2.275635533814105\n",
            "train loss:2.2642127617319106\n",
            "=== epoch:36, train acc:0.20333333333333334, test acc:0.161 ===\n",
            "train loss:2.279146012081544\n",
            "train loss:2.2675243766268007\n",
            "train loss:2.2661114116664396\n",
            "=== epoch:37, train acc:0.20333333333333334, test acc:0.1611 ===\n",
            "train loss:2.2721764784860627\n",
            "train loss:2.2675193012203705\n",
            "train loss:2.273771174984186\n",
            "=== epoch:38, train acc:0.2, test acc:0.1643 ===\n",
            "train loss:2.2575061301876884\n",
            "train loss:2.2689993914583355\n",
            "train loss:2.2733401694119526\n",
            "=== epoch:39, train acc:0.20333333333333334, test acc:0.1647 ===\n",
            "train loss:2.277145023999529\n",
            "train loss:2.2554149883412196\n",
            "train loss:2.270108060978995\n",
            "=== epoch:40, train acc:0.20666666666666667, test acc:0.1691 ===\n",
            "train loss:2.2617352508684663\n",
            "train loss:2.2640874740665655\n",
            "train loss:2.2607910831795146\n",
            "=== epoch:41, train acc:0.20666666666666667, test acc:0.1715 ===\n",
            "train loss:2.28062053260804\n",
            "train loss:2.2686960577887225\n",
            "train loss:2.2536922590040005\n",
            "=== epoch:42, train acc:0.22, test acc:0.1741 ===\n",
            "train loss:2.269703861306053\n",
            "train loss:2.253090758669656\n",
            "train loss:2.2712213639923498\n",
            "=== epoch:43, train acc:0.21666666666666667, test acc:0.1749 ===\n",
            "train loss:2.2600678171293125\n",
            "train loss:2.254815621962108\n",
            "train loss:2.2498939336607866\n",
            "=== epoch:44, train acc:0.22, test acc:0.1766 ===\n",
            "train loss:2.264794728353485\n",
            "train loss:2.2558266846606125\n",
            "train loss:2.2642906854203733\n",
            "=== epoch:45, train acc:0.22333333333333333, test acc:0.176 ===\n",
            "train loss:2.2623475411553637\n",
            "train loss:2.268099322712305\n",
            "train loss:2.2461913294533296\n",
            "=== epoch:46, train acc:0.21666666666666667, test acc:0.1747 ===\n",
            "train loss:2.2663904523325273\n",
            "train loss:2.232531284390014\n",
            "train loss:2.25304672887835\n",
            "=== epoch:47, train acc:0.22, test acc:0.1741 ===\n",
            "train loss:2.267692687195958\n",
            "train loss:2.2669248693135615\n",
            "train loss:2.256182198859994\n",
            "=== epoch:48, train acc:0.22333333333333333, test acc:0.1776 ===\n",
            "train loss:2.2636044413579266\n",
            "train loss:2.2691166940726473\n",
            "train loss:2.2694223837966803\n",
            "=== epoch:49, train acc:0.22333333333333333, test acc:0.1795 ===\n",
            "train loss:2.2515718963891964\n",
            "train loss:2.260401875614065\n",
            "train loss:2.2531159820922984\n",
            "=== epoch:50, train acc:0.23, test acc:0.1811 ===\n",
            "train loss:2.2481904206215804\n",
            "train loss:2.257884676788388\n",
            "train loss:2.2481091809359093\n",
            "=== epoch:51, train acc:0.23666666666666666, test acc:0.1814 ===\n",
            "train loss:2.2397598840124155\n",
            "train loss:2.247439886373862\n",
            "train loss:2.2577554283025036\n",
            "=== epoch:52, train acc:0.23666666666666666, test acc:0.1825 ===\n",
            "train loss:2.257825610338186\n",
            "train loss:2.258291851169442\n",
            "train loss:2.2737381803835914\n",
            "=== epoch:53, train acc:0.25, test acc:0.1855 ===\n",
            "train loss:2.263561948643618\n",
            "train loss:2.256721100362135\n",
            "train loss:2.2573747650453275\n",
            "=== epoch:54, train acc:0.25, test acc:0.1877 ===\n",
            "train loss:2.245216489890873\n",
            "train loss:2.2376404830023127\n",
            "train loss:2.232905844892519\n",
            "=== epoch:55, train acc:0.23333333333333334, test acc:0.1863 ===\n",
            "train loss:2.250547739048659\n",
            "train loss:2.242177017034577\n",
            "train loss:2.249849930813523\n",
            "=== epoch:56, train acc:0.23666666666666666, test acc:0.1882 ===\n",
            "train loss:2.244067255747694\n",
            "train loss:2.2079906162474696\n",
            "train loss:2.245628152137162\n",
            "=== epoch:57, train acc:0.22666666666666666, test acc:0.1894 ===\n",
            "train loss:2.241274822450623\n",
            "train loss:2.2549199559277078\n",
            "train loss:2.2343211195751356\n",
            "=== epoch:58, train acc:0.22666666666666666, test acc:0.1886 ===\n",
            "train loss:2.245490555052406\n",
            "train loss:2.230517487130832\n",
            "train loss:2.215798461734647\n",
            "=== epoch:59, train acc:0.22666666666666666, test acc:0.1882 ===\n",
            "train loss:2.2368425763769335\n",
            "train loss:2.2186645773769773\n",
            "train loss:2.2478097667255668\n",
            "=== epoch:60, train acc:0.23333333333333334, test acc:0.1884 ===\n",
            "train loss:2.2438603133405244\n",
            "train loss:2.235295107698339\n",
            "train loss:2.243959012074294\n",
            "=== epoch:61, train acc:0.23333333333333334, test acc:0.1897 ===\n",
            "train loss:2.2380357986849533\n",
            "train loss:2.2301531769857332\n",
            "train loss:2.2415896864980698\n",
            "=== epoch:62, train acc:0.23333333333333334, test acc:0.1891 ===\n",
            "train loss:2.2600863285227843\n",
            "train loss:2.230597459393595\n",
            "train loss:2.211875912491704\n",
            "=== epoch:63, train acc:0.23333333333333334, test acc:0.1909 ===\n",
            "train loss:2.22424875777208\n",
            "train loss:2.2222114530266457\n",
            "train loss:2.23974248640457\n",
            "=== epoch:64, train acc:0.23333333333333334, test acc:0.1919 ===\n",
            "train loss:2.2558496569210225\n",
            "train loss:2.2295847348356603\n",
            "train loss:2.226818402100817\n",
            "=== epoch:65, train acc:0.23333333333333334, test acc:0.1928 ===\n",
            "train loss:2.229943351181318\n",
            "train loss:2.240263680503325\n",
            "train loss:2.246605379726071\n",
            "=== epoch:66, train acc:0.23666666666666666, test acc:0.1931 ===\n",
            "train loss:2.2440995954049985\n",
            "train loss:2.2429167777658945\n",
            "train loss:2.2221228291404382\n",
            "=== epoch:67, train acc:0.23333333333333334, test acc:0.1917 ===\n",
            "train loss:2.2111821182334843\n",
            "train loss:2.2408098898240807\n",
            "train loss:2.2483922185647423\n",
            "=== epoch:68, train acc:0.23666666666666666, test acc:0.1907 ===\n",
            "train loss:2.2260759782771955\n",
            "train loss:2.223367115536432\n",
            "train loss:2.2442887195293646\n",
            "=== epoch:69, train acc:0.24, test acc:0.1937 ===\n",
            "train loss:2.2339901439565106\n",
            "train loss:2.2378770995821227\n",
            "train loss:2.2158583074984555\n",
            "=== epoch:70, train acc:0.25, test acc:0.197 ===\n",
            "train loss:2.2566642275638067\n",
            "train loss:2.2291565099123987\n",
            "train loss:2.20877185166306\n",
            "=== epoch:71, train acc:0.25333333333333335, test acc:0.1988 ===\n",
            "train loss:2.219460105597397\n",
            "train loss:2.2017684914094775\n",
            "train loss:2.2326183158339985\n",
            "=== epoch:72, train acc:0.25, test acc:0.1979 ===\n",
            "train loss:2.221384914868002\n",
            "train loss:2.240166884812537\n",
            "train loss:2.2168273472502644\n",
            "=== epoch:73, train acc:0.25333333333333335, test acc:0.2 ===\n",
            "train loss:2.24367853906771\n",
            "train loss:2.2090729579712702\n",
            "train loss:2.221371449297715\n",
            "=== epoch:74, train acc:0.25333333333333335, test acc:0.1998 ===\n",
            "train loss:2.22424037408195\n",
            "train loss:2.2259052148081975\n",
            "train loss:2.253074483171395\n",
            "=== epoch:75, train acc:0.25333333333333335, test acc:0.2005 ===\n",
            "train loss:2.2070586253923583\n",
            "train loss:2.2290778381263676\n",
            "train loss:2.2310617688762493\n",
            "=== epoch:76, train acc:0.25333333333333335, test acc:0.2017 ===\n",
            "train loss:2.2087155569196297\n",
            "train loss:2.2348086282561663\n",
            "train loss:2.217688033467777\n",
            "=== epoch:77, train acc:0.25333333333333335, test acc:0.2024 ===\n",
            "train loss:2.190206873770453\n",
            "train loss:2.2265211316142377\n",
            "train loss:2.220686431941367\n",
            "=== epoch:78, train acc:0.25333333333333335, test acc:0.2009 ===\n",
            "train loss:2.2156523554933263\n",
            "train loss:2.2096503093897297\n",
            "train loss:2.212669742253863\n",
            "=== epoch:79, train acc:0.25333333333333335, test acc:0.201 ===\n",
            "train loss:2.190599204993765\n",
            "train loss:2.2274148801429643\n",
            "train loss:2.21312984544591\n",
            "=== epoch:80, train acc:0.25333333333333335, test acc:0.2021 ===\n",
            "train loss:2.236674209998522\n",
            "train loss:2.218034551839769\n",
            "train loss:2.2480252860473593\n",
            "=== epoch:81, train acc:0.25666666666666665, test acc:0.2048 ===\n",
            "train loss:2.196755632850936\n",
            "train loss:2.209074531548727\n",
            "train loss:2.2046080208626027\n",
            "=== epoch:82, train acc:0.25666666666666665, test acc:0.2044 ===\n",
            "train loss:2.245257591181047\n",
            "train loss:2.2069285300555195\n",
            "train loss:2.1902037298064387\n",
            "=== epoch:83, train acc:0.25666666666666665, test acc:0.2058 ===\n",
            "train loss:2.1888924834903305\n",
            "train loss:2.183177615280323\n",
            "train loss:2.2080516178512353\n",
            "=== epoch:84, train acc:0.25, test acc:0.2057 ===\n",
            "train loss:2.1915690763153495\n",
            "train loss:2.21643155605993\n",
            "train loss:2.1969586709116045\n",
            "=== epoch:85, train acc:0.25333333333333335, test acc:0.2081 ===\n",
            "train loss:2.227889600453412\n",
            "train loss:2.209518362667108\n",
            "train loss:2.201131388416931\n",
            "=== epoch:86, train acc:0.25333333333333335, test acc:0.2077 ===\n",
            "train loss:2.193607805538667\n",
            "train loss:2.1779309679410805\n",
            "train loss:2.1597052687527762\n",
            "=== epoch:87, train acc:0.25333333333333335, test acc:0.2075 ===\n",
            "train loss:2.198940662973599\n",
            "train loss:2.221727981280696\n",
            "train loss:2.1934729394011714\n",
            "=== epoch:88, train acc:0.25666666666666665, test acc:0.2079 ===\n",
            "train loss:2.172323799796493\n",
            "train loss:2.1847017891455573\n",
            "train loss:2.1806496491907352\n",
            "=== epoch:89, train acc:0.25666666666666665, test acc:0.2073 ===\n",
            "train loss:2.1969370569318016\n",
            "train loss:2.202014590080834\n",
            "train loss:2.175737952485973\n",
            "=== epoch:90, train acc:0.26, test acc:0.2093 ===\n",
            "train loss:2.1727578706530717\n",
            "train loss:2.1831219511316027\n",
            "train loss:2.209641944133283\n",
            "=== epoch:91, train acc:0.25666666666666665, test acc:0.209 ===\n",
            "train loss:2.1851553270194386\n",
            "train loss:2.1849457211668546\n",
            "train loss:2.1938688205295747\n",
            "=== epoch:92, train acc:0.25333333333333335, test acc:0.2094 ===\n",
            "train loss:2.2235616373016187\n",
            "train loss:2.159160696349175\n",
            "train loss:2.204582061433484\n",
            "=== epoch:93, train acc:0.26, test acc:0.2104 ===\n",
            "train loss:2.174298727428933\n",
            "train loss:2.1577698729248587\n",
            "train loss:2.206328196905383\n",
            "=== epoch:94, train acc:0.26, test acc:0.2117 ===\n",
            "train loss:2.1765911762490457\n",
            "train loss:2.1746537612244325\n",
            "train loss:2.1849948262988037\n",
            "=== epoch:95, train acc:0.26, test acc:0.2105 ===\n",
            "train loss:2.190607801167392\n",
            "train loss:2.1815486338894927\n",
            "train loss:2.151826395345123\n",
            "=== epoch:96, train acc:0.25666666666666665, test acc:0.2102 ===\n",
            "train loss:2.129394511021757\n",
            "train loss:2.1870325848406935\n",
            "train loss:2.1571580361347586\n",
            "=== epoch:97, train acc:0.26, test acc:0.2097 ===\n",
            "train loss:2.2062913615867146\n",
            "train loss:2.1824832010803634\n",
            "train loss:2.1591402021258195\n",
            "=== epoch:98, train acc:0.25666666666666665, test acc:0.2117 ===\n",
            "train loss:2.1829519390925642\n",
            "train loss:2.155228865701091\n",
            "train loss:2.1653552708950343\n",
            "=== epoch:99, train acc:0.25333333333333335, test acc:0.211 ===\n",
            "train loss:2.213259127564713\n",
            "train loss:2.1882153836548217\n",
            "train loss:2.20883485817027\n",
            "=== epoch:100, train acc:0.26666666666666666, test acc:0.2154 ===\n",
            "train loss:2.168502677847387\n",
            "train loss:2.2081151790749036\n",
            "train loss:2.1907932348249024\n",
            "=== epoch:101, train acc:0.26666666666666666, test acc:0.218 ===\n",
            "train loss:2.146069468211895\n",
            "train loss:2.1645466011007577\n",
            "train loss:2.181876252662737\n",
            "=== epoch:102, train acc:0.26666666666666666, test acc:0.2186 ===\n",
            "train loss:2.179218218422773\n",
            "train loss:2.1640089517254504\n",
            "train loss:2.164818969599894\n",
            "=== epoch:103, train acc:0.26666666666666666, test acc:0.2203 ===\n",
            "train loss:2.1628384415652433\n",
            "train loss:2.1497434325647506\n",
            "train loss:2.1921206028148172\n",
            "=== epoch:104, train acc:0.26666666666666666, test acc:0.2187 ===\n",
            "train loss:2.1849502345974154\n",
            "train loss:2.172778465921583\n",
            "train loss:2.1850121085893974\n",
            "=== epoch:105, train acc:0.26666666666666666, test acc:0.2223 ===\n",
            "train loss:2.1789849240458024\n",
            "train loss:2.175952976915438\n",
            "train loss:2.163671905757909\n",
            "=== epoch:106, train acc:0.27, test acc:0.2236 ===\n",
            "train loss:2.161705711468102\n",
            "train loss:2.1538373504605217\n",
            "train loss:2.1566739599534146\n",
            "=== epoch:107, train acc:0.26666666666666666, test acc:0.2256 ===\n",
            "train loss:2.1614237695273553\n",
            "train loss:2.15583963651282\n",
            "train loss:2.144434738169121\n",
            "=== epoch:108, train acc:0.27, test acc:0.2261 ===\n",
            "train loss:2.1775606209468474\n",
            "train loss:2.1754767432016693\n",
            "train loss:2.157809738458297\n",
            "=== epoch:109, train acc:0.27, test acc:0.2303 ===\n",
            "train loss:2.183769347072088\n",
            "train loss:2.14105237911226\n",
            "train loss:2.1585266390917064\n",
            "=== epoch:110, train acc:0.2733333333333333, test acc:0.2305 ===\n",
            "train loss:2.162671930597979\n",
            "train loss:2.19660950955752\n",
            "train loss:2.1504619266090343\n",
            "=== epoch:111, train acc:0.28, test acc:0.2329 ===\n",
            "train loss:2.1469395226112216\n",
            "train loss:2.134978221878264\n",
            "train loss:2.1291961569070903\n",
            "=== epoch:112, train acc:0.28, test acc:0.2317 ===\n",
            "train loss:2.145721509182261\n",
            "train loss:2.160827647989572\n",
            "train loss:2.155301319493113\n",
            "=== epoch:113, train acc:0.28, test acc:0.2307 ===\n",
            "train loss:2.0954396370949926\n",
            "train loss:2.1815653141014137\n",
            "train loss:2.1466315429769285\n",
            "=== epoch:114, train acc:0.2833333333333333, test acc:0.2318 ===\n",
            "train loss:2.1040976571025745\n",
            "train loss:2.166950924511499\n",
            "train loss:2.137403225208043\n",
            "=== epoch:115, train acc:0.29, test acc:0.2333 ===\n",
            "train loss:2.159960154374509\n",
            "train loss:2.1582441202910854\n",
            "train loss:2.1290404141896078\n",
            "=== epoch:116, train acc:0.29, test acc:0.233 ===\n",
            "train loss:2.159753268112089\n",
            "train loss:2.1411870470931307\n",
            "train loss:2.1826578863825037\n",
            "=== epoch:117, train acc:0.3, test acc:0.2393 ===\n",
            "train loss:2.1885643798624734\n",
            "train loss:2.111213592390956\n",
            "train loss:2.1105693504731566\n",
            "=== epoch:118, train acc:0.3, test acc:0.2396 ===\n",
            "train loss:2.132144158457568\n",
            "train loss:2.1054890873785195\n",
            "train loss:2.1376707502261763\n",
            "=== epoch:119, train acc:0.30333333333333334, test acc:0.2431 ===\n",
            "train loss:2.130461029948082\n",
            "train loss:2.12054213272151\n",
            "train loss:2.177149729747675\n",
            "=== epoch:120, train acc:0.31, test acc:0.2492 ===\n",
            "train loss:2.141730683208957\n",
            "train loss:2.0280216215991445\n",
            "train loss:2.0913819628023895\n",
            "=== epoch:121, train acc:0.30666666666666664, test acc:0.2448 ===\n",
            "train loss:2.088329398520603\n",
            "train loss:2.089127165726843\n",
            "train loss:2.108493128947846\n",
            "=== epoch:122, train acc:0.31333333333333335, test acc:0.245 ===\n",
            "train loss:2.1895176724321037\n",
            "train loss:2.150354104132255\n",
            "train loss:2.1065833763928783\n",
            "=== epoch:123, train acc:0.32, test acc:0.2482 ===\n",
            "train loss:2.127612687289213\n",
            "train loss:2.0918612193229777\n",
            "train loss:2.1197026860181936\n",
            "=== epoch:124, train acc:0.33, test acc:0.2515 ===\n",
            "train loss:2.0828393479126404\n",
            "train loss:2.114176276987995\n",
            "train loss:2.077098129137141\n",
            "=== epoch:125, train acc:0.33, test acc:0.251 ===\n",
            "train loss:2.073777665835875\n",
            "train loss:2.1183837154196876\n",
            "train loss:2.1010424866697193\n",
            "=== epoch:126, train acc:0.33, test acc:0.255 ===\n",
            "train loss:2.1710900746093293\n",
            "train loss:2.1245797837311358\n",
            "train loss:2.049689313713092\n",
            "=== epoch:127, train acc:0.32666666666666666, test acc:0.2505 ===\n",
            "train loss:2.1145984277495846\n",
            "train loss:2.1100472422608787\n",
            "train loss:2.1122867001299825\n",
            "=== epoch:128, train acc:0.32666666666666666, test acc:0.2522 ===\n",
            "train loss:2.0597585174634827\n",
            "train loss:2.107439452096071\n",
            "train loss:2.090074569379309\n",
            "=== epoch:129, train acc:0.32666666666666666, test acc:0.2544 ===\n",
            "train loss:2.0850905121930365\n",
            "train loss:2.0847031691098565\n",
            "train loss:2.065768814932601\n",
            "=== epoch:130, train acc:0.33666666666666667, test acc:0.2588 ===\n",
            "train loss:2.0975450515387757\n",
            "train loss:2.1334758805403333\n",
            "train loss:2.096792284434973\n",
            "=== epoch:131, train acc:0.3333333333333333, test acc:0.2597 ===\n",
            "train loss:2.070647664309182\n",
            "train loss:2.0594124687055895\n",
            "train loss:2.082767633840293\n",
            "=== epoch:132, train acc:0.33666666666666667, test acc:0.2606 ===\n",
            "train loss:2.083214880343171\n",
            "train loss:2.1032662106019506\n",
            "train loss:2.0409297346563484\n",
            "=== epoch:133, train acc:0.3433333333333333, test acc:0.263 ===\n",
            "train loss:2.121118751389212\n",
            "train loss:2.0332260121720784\n",
            "train loss:2.090849836584829\n",
            "=== epoch:134, train acc:0.3466666666666667, test acc:0.2643 ===\n",
            "train loss:2.1397392425385178\n",
            "train loss:2.0653422695405714\n",
            "train loss:2.0877932174355838\n",
            "=== epoch:135, train acc:0.36333333333333334, test acc:0.267 ===\n",
            "train loss:2.1166598086621558\n",
            "train loss:2.0972404162256537\n",
            "train loss:2.0741527521512313\n",
            "=== epoch:136, train acc:0.37, test acc:0.2703 ===\n",
            "train loss:2.053752467709146\n",
            "train loss:2.0586062416685906\n",
            "train loss:2.041134982804748\n",
            "=== epoch:137, train acc:0.36, test acc:0.2691 ===\n",
            "train loss:1.9974710433299003\n",
            "train loss:2.0870179024402615\n",
            "train loss:2.0940288893933614\n",
            "=== epoch:138, train acc:0.36333333333333334, test acc:0.2711 ===\n",
            "train loss:2.03267120416919\n",
            "train loss:2.0509535486235073\n",
            "train loss:2.033593220810594\n",
            "=== epoch:139, train acc:0.36333333333333334, test acc:0.2717 ===\n",
            "train loss:2.039101495450496\n",
            "train loss:2.0600846230814223\n",
            "train loss:2.0206017169676924\n",
            "=== epoch:140, train acc:0.36666666666666664, test acc:0.2728 ===\n",
            "train loss:2.0773390671701453\n",
            "train loss:2.0487543527955547\n",
            "train loss:2.09123808254643\n",
            "=== epoch:141, train acc:0.36333333333333334, test acc:0.2741 ===\n",
            "train loss:2.111615679835283\n",
            "train loss:2.059711275824123\n",
            "train loss:2.012463383561732\n",
            "=== epoch:142, train acc:0.37666666666666665, test acc:0.2796 ===\n",
            "train loss:2.0225651694990265\n",
            "train loss:2.0438876358281406\n",
            "train loss:2.0736843468057047\n",
            "=== epoch:143, train acc:0.37333333333333335, test acc:0.2793 ===\n",
            "train loss:1.9714301017124694\n",
            "train loss:2.0895808884149814\n",
            "train loss:2.066644303484365\n",
            "=== epoch:144, train acc:0.36333333333333334, test acc:0.2782 ===\n",
            "train loss:2.053411134671043\n",
            "train loss:2.084568689146078\n",
            "train loss:2.0775944256297096\n",
            "=== epoch:145, train acc:0.36, test acc:0.279 ===\n",
            "train loss:2.00247274567177\n",
            "train loss:2.0659335162267594\n",
            "train loss:1.9671652047138806\n",
            "=== epoch:146, train acc:0.36, test acc:0.2753 ===\n",
            "train loss:1.9626248443660412\n",
            "train loss:2.0892690650865866\n",
            "train loss:1.9218167971148625\n",
            "=== epoch:147, train acc:0.3566666666666667, test acc:0.2735 ===\n",
            "train loss:2.040000854688661\n",
            "train loss:2.0083024924049995\n",
            "train loss:2.0017342488298313\n",
            "=== epoch:148, train acc:0.36, test acc:0.2766 ===\n",
            "train loss:2.0451373486847366\n",
            "train loss:2.082670904692243\n",
            "train loss:2.092146912644252\n",
            "=== epoch:149, train acc:0.36, test acc:0.279 ===\n",
            "train loss:2.0527837489630794\n",
            "train loss:2.0430439190003535\n",
            "train loss:2.0556821508681384\n",
            "=== epoch:150, train acc:0.36333333333333334, test acc:0.28 ===\n",
            "train loss:1.9879520503536303\n",
            "train loss:2.0539832231163446\n",
            "train loss:2.0225008784503693\n",
            "=== epoch:151, train acc:0.36333333333333334, test acc:0.2817 ===\n",
            "train loss:2.015371243861226\n",
            "train loss:1.980241713623294\n",
            "train loss:1.9920464148239538\n",
            "=== epoch:152, train acc:0.36666666666666664, test acc:0.2825 ===\n",
            "train loss:2.0417048387321\n",
            "train loss:1.9537039331260493\n",
            "train loss:1.9725936174467114\n",
            "=== epoch:153, train acc:0.36666666666666664, test acc:0.2855 ===\n",
            "train loss:1.936970616924185\n",
            "train loss:1.9542499717022057\n",
            "train loss:2.0465953653767555\n",
            "=== epoch:154, train acc:0.36333333333333334, test acc:0.2869 ===\n",
            "train loss:2.0474328021942525\n",
            "train loss:2.0537638718167512\n",
            "train loss:1.9764503818257086\n",
            "=== epoch:155, train acc:0.36, test acc:0.29 ===\n",
            "train loss:1.9650050173341123\n",
            "train loss:1.9855637576704004\n",
            "train loss:2.001575686742608\n",
            "=== epoch:156, train acc:0.36, test acc:0.2914 ===\n",
            "train loss:1.966036150538103\n",
            "train loss:1.9288186979863002\n",
            "train loss:1.9686108248834893\n",
            "=== epoch:157, train acc:0.36, test acc:0.2913 ===\n",
            "train loss:1.991113786152284\n",
            "train loss:2.0397204684087886\n",
            "train loss:1.91424663025057\n",
            "=== epoch:158, train acc:0.36333333333333334, test acc:0.2917 ===\n",
            "train loss:1.8742341719724414\n",
            "train loss:1.982574564663951\n",
            "train loss:1.9457942164936852\n",
            "=== epoch:159, train acc:0.36666666666666664, test acc:0.2937 ===\n",
            "train loss:1.9605595486613456\n",
            "train loss:2.0116799992181824\n",
            "train loss:2.101506610093697\n",
            "=== epoch:160, train acc:0.37333333333333335, test acc:0.2956 ===\n",
            "train loss:2.0090695269750993\n",
            "train loss:1.9526859172923532\n",
            "train loss:1.9439052465163158\n",
            "=== epoch:161, train acc:0.37, test acc:0.2934 ===\n",
            "train loss:1.987914000980291\n",
            "train loss:1.9962141393499224\n",
            "train loss:1.9769418929002824\n",
            "=== epoch:162, train acc:0.36, test acc:0.2946 ===\n",
            "train loss:1.9394085265616916\n",
            "train loss:1.9738723879584825\n",
            "train loss:1.9871900109907263\n",
            "=== epoch:163, train acc:0.37, test acc:0.2948 ===\n",
            "train loss:1.967200666160171\n",
            "train loss:1.9779007053462805\n",
            "train loss:1.9094988839501688\n",
            "=== epoch:164, train acc:0.37, test acc:0.2974 ===\n",
            "train loss:1.9558892893943771\n",
            "train loss:1.899031653086302\n",
            "train loss:1.8945673931109994\n",
            "=== epoch:165, train acc:0.37, test acc:0.2984 ===\n",
            "train loss:1.9022744823552384\n",
            "train loss:1.8515005694209148\n",
            "train loss:2.0108660791241575\n",
            "=== epoch:166, train acc:0.37, test acc:0.2963 ===\n",
            "train loss:1.8414548270437014\n",
            "train loss:2.0402425039611667\n",
            "train loss:1.9652812494057152\n",
            "=== epoch:167, train acc:0.36666666666666664, test acc:0.2948 ===\n",
            "train loss:2.025617863170086\n",
            "train loss:1.9256074415721816\n",
            "train loss:1.937980217569744\n",
            "=== epoch:168, train acc:0.36333333333333334, test acc:0.2935 ===\n",
            "train loss:1.912033196159183\n",
            "train loss:1.8402772840878447\n",
            "train loss:2.0451923715091604\n",
            "=== epoch:169, train acc:0.36666666666666664, test acc:0.2952 ===\n",
            "train loss:1.963028266000564\n",
            "train loss:1.9633697637278065\n",
            "train loss:1.9050217829461575\n",
            "=== epoch:170, train acc:0.37, test acc:0.2965 ===\n",
            "train loss:1.9523836246433222\n",
            "train loss:1.861164981608587\n",
            "train loss:1.9926537197376595\n",
            "=== epoch:171, train acc:0.36666666666666664, test acc:0.2965 ===\n",
            "train loss:1.8048518117558592\n",
            "train loss:1.9938706859607873\n",
            "train loss:1.993706815174882\n",
            "=== epoch:172, train acc:0.36, test acc:0.2965 ===\n",
            "train loss:1.9547805784179433\n",
            "train loss:1.941602189356516\n",
            "train loss:1.8251894586620674\n",
            "=== epoch:173, train acc:0.36666666666666664, test acc:0.299 ===\n",
            "train loss:1.9679888710103355\n",
            "train loss:1.897516258596918\n",
            "train loss:1.834745398357148\n",
            "=== epoch:174, train acc:0.37, test acc:0.2994 ===\n",
            "train loss:1.862479368042454\n",
            "train loss:1.9649353544767436\n",
            "train loss:1.8755773920701855\n",
            "=== epoch:175, train acc:0.36333333333333334, test acc:0.2986 ===\n",
            "train loss:1.9524747846304362\n",
            "train loss:1.8419770511035083\n",
            "train loss:1.8867110913728231\n",
            "=== epoch:176, train acc:0.36666666666666664, test acc:0.3003 ===\n",
            "train loss:1.9568370146471765\n",
            "train loss:1.9049755280312233\n",
            "train loss:1.9165817623582972\n",
            "=== epoch:177, train acc:0.36333333333333334, test acc:0.3002 ===\n",
            "train loss:1.9110502138419472\n",
            "train loss:1.934396909971556\n",
            "train loss:1.875483230907038\n",
            "=== epoch:178, train acc:0.36666666666666664, test acc:0.3007 ===\n",
            "train loss:1.999080831314547\n",
            "train loss:1.8145784420113802\n",
            "train loss:1.8954387800436725\n",
            "=== epoch:179, train acc:0.37, test acc:0.3038 ===\n",
            "train loss:1.8969966181415592\n",
            "train loss:1.8727097895370102\n",
            "train loss:1.8926392542437371\n",
            "=== epoch:180, train acc:0.37333333333333335, test acc:0.3043 ===\n",
            "train loss:1.8616092796847843\n",
            "train loss:1.8335410386710476\n",
            "train loss:1.8475505841745092\n",
            "=== epoch:181, train acc:0.37, test acc:0.3027 ===\n",
            "train loss:1.849549348295653\n",
            "train loss:1.8570795510294846\n",
            "train loss:1.8883296115931174\n",
            "=== epoch:182, train acc:0.37333333333333335, test acc:0.3043 ===\n",
            "train loss:1.915166174601992\n",
            "train loss:1.9021240153828487\n",
            "train loss:1.959125077523955\n",
            "=== epoch:183, train acc:0.37333333333333335, test acc:0.3045 ===\n",
            "train loss:1.8453175385087592\n",
            "train loss:1.9099365135838537\n",
            "train loss:1.9261147120876962\n",
            "=== epoch:184, train acc:0.37333333333333335, test acc:0.3071 ===\n",
            "train loss:1.87197708883902\n",
            "train loss:1.9200879380081022\n",
            "train loss:1.920532773032543\n",
            "=== epoch:185, train acc:0.37333333333333335, test acc:0.3078 ===\n",
            "train loss:1.7201564870051929\n",
            "train loss:1.844645907800394\n",
            "train loss:1.7435012846581852\n",
            "=== epoch:186, train acc:0.37666666666666665, test acc:0.3084 ===\n",
            "train loss:1.9447151437232488\n",
            "train loss:1.8860059004847085\n",
            "train loss:1.8741486448692817\n",
            "=== epoch:187, train acc:0.38333333333333336, test acc:0.3116 ===\n",
            "train loss:1.9031367204847731\n",
            "train loss:1.9182556448905297\n",
            "train loss:1.7854013289148876\n",
            "=== epoch:188, train acc:0.38, test acc:0.3135 ===\n",
            "train loss:1.9151178150387236\n",
            "train loss:1.9085437157234544\n",
            "train loss:1.8924126679421485\n",
            "=== epoch:189, train acc:0.37666666666666665, test acc:0.3135 ===\n",
            "train loss:1.8840536259506624\n",
            "train loss:1.8922528670201673\n",
            "train loss:1.7916455554535586\n",
            "=== epoch:190, train acc:0.38333333333333336, test acc:0.3151 ===\n",
            "train loss:1.8263031931283757\n",
            "train loss:1.8347427977205886\n",
            "train loss:1.9228147333169554\n",
            "=== epoch:191, train acc:0.37666666666666665, test acc:0.3147 ===\n",
            "train loss:1.7942555622018401\n",
            "train loss:1.797749820714596\n",
            "train loss:1.8914989657331787\n",
            "=== epoch:192, train acc:0.37666666666666665, test acc:0.3137 ===\n",
            "train loss:1.8990375429624131\n",
            "train loss:1.7689040655007782\n",
            "train loss:1.8257253318536937\n",
            "=== epoch:193, train acc:0.37666666666666665, test acc:0.3123 ===\n",
            "train loss:1.862424903569624\n",
            "train loss:1.877781595419143\n",
            "train loss:1.922558876440207\n",
            "=== epoch:194, train acc:0.38, test acc:0.3133 ===\n",
            "train loss:1.7628442397889947\n",
            "train loss:1.809439696796131\n",
            "train loss:1.89515005978745\n",
            "=== epoch:195, train acc:0.38, test acc:0.3163 ===\n",
            "train loss:1.8331313290823288\n",
            "train loss:1.8670316182179396\n",
            "train loss:1.8702691748349676\n",
            "=== epoch:196, train acc:0.38333333333333336, test acc:0.318 ===\n",
            "train loss:1.810821103098449\n",
            "train loss:1.8463998704240885\n",
            "train loss:1.796630066475173\n",
            "=== epoch:197, train acc:0.38, test acc:0.3182 ===\n",
            "train loss:1.892110037927399\n",
            "train loss:1.8037151869251145\n",
            "train loss:1.7751300996536943\n",
            "=== epoch:198, train acc:0.38666666666666666, test acc:0.3179 ===\n",
            "train loss:1.735307589831491\n",
            "train loss:1.8198112551781105\n",
            "train loss:1.834577621518288\n",
            "=== epoch:199, train acc:0.3933333333333333, test acc:0.3203 ===\n",
            "train loss:1.8216502665006624\n",
            "train loss:1.8058425559522897\n",
            "train loss:1.876535974853208\n",
            "=== epoch:200, train acc:0.39, test acc:0.3208 ===\n",
            "train loss:1.8361678187071613\n",
            "train loss:1.8805763747239128\n",
            "train loss:1.7971928254509613\n",
            "=== epoch:201, train acc:0.38666666666666666, test acc:0.3224 ===\n",
            "train loss:1.8330371508685623\n",
            "train loss:1.802433816328358\n",
            "train loss:1.8699939838465074\n",
            "=== epoch:202, train acc:0.39, test acc:0.3212 ===\n",
            "train loss:1.7210061925009805\n",
            "train loss:1.8052054640218522\n",
            "train loss:1.8689314039368774\n",
            "=== epoch:203, train acc:0.38666666666666666, test acc:0.3203 ===\n",
            "train loss:1.8102961685570218\n",
            "train loss:1.7450343955279966\n",
            "train loss:1.7902879068245958\n",
            "=== epoch:204, train acc:0.3933333333333333, test acc:0.3227 ===\n",
            "train loss:1.84065837425331\n",
            "train loss:1.8168915681702942\n",
            "train loss:1.6567080673776058\n",
            "=== epoch:205, train acc:0.3933333333333333, test acc:0.322 ===\n",
            "train loss:1.7695141647179171\n",
            "train loss:1.7640680319871151\n",
            "train loss:1.6621074805985572\n",
            "=== epoch:206, train acc:0.39666666666666667, test acc:0.3224 ===\n",
            "train loss:1.6959412967573542\n",
            "train loss:1.8773233730835739\n",
            "train loss:1.6322751924266783\n",
            "=== epoch:207, train acc:0.39, test acc:0.3221 ===\n",
            "train loss:1.6290466826529817\n",
            "train loss:1.6829187412052087\n",
            "train loss:1.7124580756515975\n",
            "=== epoch:208, train acc:0.39666666666666667, test acc:0.3222 ===\n",
            "train loss:1.6773968507497738\n",
            "train loss:1.7229381040492588\n",
            "train loss:1.7110561398868436\n",
            "=== epoch:209, train acc:0.4, test acc:0.3282 ===\n",
            "train loss:1.7083845988045574\n",
            "train loss:1.7494665679453212\n",
            "train loss:1.6985041119989313\n",
            "=== epoch:210, train acc:0.4033333333333333, test acc:0.3287 ===\n",
            "train loss:1.8576144214567591\n",
            "train loss:1.7540938308303395\n",
            "train loss:1.745682129806503\n",
            "=== epoch:211, train acc:0.4033333333333333, test acc:0.3275 ===\n",
            "train loss:1.7350209835011876\n",
            "train loss:1.7175732576503486\n",
            "train loss:1.553931551437661\n",
            "=== epoch:212, train acc:0.4033333333333333, test acc:0.3275 ===\n",
            "train loss:1.6891515836059385\n",
            "train loss:1.7590476919540128\n",
            "train loss:1.6927177136462683\n",
            "=== epoch:213, train acc:0.4033333333333333, test acc:0.3292 ===\n",
            "train loss:1.6831619365003148\n",
            "train loss:1.6945290644624615\n",
            "train loss:1.655182608285297\n",
            "=== epoch:214, train acc:0.42, test acc:0.3351 ===\n",
            "train loss:1.7271446610781929\n",
            "train loss:1.7433632929436766\n",
            "train loss:1.7381937451898062\n",
            "=== epoch:215, train acc:0.42, test acc:0.3378 ===\n",
            "train loss:1.671115751777501\n",
            "train loss:1.6349698338045089\n",
            "train loss:1.6749977123467041\n",
            "=== epoch:216, train acc:0.42, test acc:0.3374 ===\n",
            "train loss:1.6168450747616385\n",
            "train loss:1.8222895608595695\n",
            "train loss:1.6342206449138905\n",
            "=== epoch:217, train acc:0.42, test acc:0.3394 ===\n",
            "train loss:1.822143164919472\n",
            "train loss:1.665701150342242\n",
            "train loss:1.6487932507365413\n",
            "=== epoch:218, train acc:0.41333333333333333, test acc:0.3332 ===\n",
            "train loss:1.7766840035702933\n",
            "train loss:1.6462516687634379\n",
            "train loss:1.77787579663006\n",
            "=== epoch:219, train acc:0.42333333333333334, test acc:0.3384 ===\n",
            "train loss:1.7420928674024048\n",
            "train loss:1.7785639805376103\n",
            "train loss:1.724472554380785\n",
            "=== epoch:220, train acc:0.42333333333333334, test acc:0.338 ===\n",
            "train loss:1.760121879255833\n",
            "train loss:1.5543702519894635\n",
            "train loss:1.6137069802921031\n",
            "=== epoch:221, train acc:0.4266666666666667, test acc:0.341 ===\n",
            "train loss:1.6566686582949262\n",
            "train loss:1.6266551481752627\n",
            "train loss:1.6747531036014272\n",
            "=== epoch:222, train acc:0.4266666666666667, test acc:0.3407 ===\n",
            "train loss:1.6693341434193654\n",
            "train loss:1.6941523935072849\n",
            "train loss:1.5838433414368098\n",
            "=== epoch:223, train acc:0.43333333333333335, test acc:0.3428 ===\n",
            "train loss:1.7283559174427088\n",
            "train loss:1.6021166531631894\n",
            "train loss:1.6227010633972043\n",
            "=== epoch:224, train acc:0.43333333333333335, test acc:0.3457 ===\n",
            "train loss:1.7431065293458945\n",
            "train loss:1.6846692789610258\n",
            "train loss:1.5986671251387463\n",
            "=== epoch:225, train acc:0.43, test acc:0.3475 ===\n",
            "train loss:1.702703050632465\n",
            "train loss:1.690171378283585\n",
            "train loss:1.664362863177822\n",
            "=== epoch:226, train acc:0.43333333333333335, test acc:0.347 ===\n",
            "train loss:1.6602004330247073\n",
            "train loss:1.5090159199705575\n",
            "train loss:1.6194124127397387\n",
            "=== epoch:227, train acc:0.43666666666666665, test acc:0.3466 ===\n",
            "train loss:1.6690609471509275\n",
            "train loss:1.6614845180262252\n",
            "train loss:1.7173143900634014\n",
            "=== epoch:228, train acc:0.43333333333333335, test acc:0.3499 ===\n",
            "train loss:1.665745093479386\n",
            "train loss:1.6663366162013709\n",
            "train loss:1.4559877171993005\n",
            "=== epoch:229, train acc:0.44666666666666666, test acc:0.3554 ===\n",
            "train loss:1.6040919263996798\n",
            "train loss:1.6292484776162295\n",
            "train loss:1.5879081929152912\n",
            "=== epoch:230, train acc:0.44666666666666666, test acc:0.3555 ===\n",
            "train loss:1.6196707747709393\n",
            "train loss:1.6937423832058718\n",
            "train loss:1.4992701815651914\n",
            "=== epoch:231, train acc:0.44333333333333336, test acc:0.3562 ===\n",
            "train loss:1.6869938299671474\n",
            "train loss:1.4674399082170575\n",
            "train loss:1.6958392061551095\n",
            "=== epoch:232, train acc:0.44666666666666666, test acc:0.3594 ===\n",
            "train loss:1.6403581830554135\n",
            "train loss:1.4794368203375838\n",
            "train loss:1.6081447290754054\n",
            "=== epoch:233, train acc:0.44666666666666666, test acc:0.36 ===\n",
            "train loss:1.6926338875582576\n",
            "train loss:1.585567761562405\n",
            "train loss:1.5761533031968953\n",
            "=== epoch:234, train acc:0.4533333333333333, test acc:0.3612 ===\n",
            "train loss:1.5777240572868205\n",
            "train loss:1.670685427886585\n",
            "train loss:1.4486343487748463\n",
            "=== epoch:235, train acc:0.4533333333333333, test acc:0.3594 ===\n",
            "train loss:1.6794998047705443\n",
            "train loss:1.524318778982887\n",
            "train loss:1.5787590001496599\n",
            "=== epoch:236, train acc:0.46, test acc:0.3633 ===\n",
            "train loss:1.570356248676448\n",
            "train loss:1.5150429763840103\n",
            "train loss:1.637512388403274\n",
            "=== epoch:237, train acc:0.4533333333333333, test acc:0.3641 ===\n",
            "train loss:1.6139347403619322\n",
            "train loss:1.5507776614061828\n",
            "train loss:1.6026631955989867\n",
            "=== epoch:238, train acc:0.4633333333333333, test acc:0.367 ===\n",
            "train loss:1.615489801547409\n",
            "train loss:1.4656369408989767\n",
            "train loss:1.7233691341369168\n",
            "=== epoch:239, train acc:0.46, test acc:0.3686 ===\n",
            "train loss:1.6277601778808821\n",
            "train loss:1.4258908952053624\n",
            "train loss:1.5323597284561452\n",
            "=== epoch:240, train acc:0.4666666666666667, test acc:0.3704 ===\n",
            "train loss:1.5595360059025183\n",
            "train loss:1.4636583649109693\n",
            "train loss:1.4717278043555313\n",
            "=== epoch:241, train acc:0.47333333333333333, test acc:0.3705 ===\n",
            "train loss:1.5316128110625937\n",
            "train loss:1.6656847969733712\n",
            "train loss:1.5663020339404405\n",
            "=== epoch:242, train acc:0.47, test acc:0.37 ===\n",
            "train loss:1.4430294296205581\n",
            "train loss:1.4907299737988444\n",
            "train loss:1.4158369760441158\n",
            "=== epoch:243, train acc:0.4766666666666667, test acc:0.3712 ===\n",
            "train loss:1.4964021360814628\n",
            "train loss:1.5117048492298792\n",
            "train loss:1.5845429330069596\n",
            "=== epoch:244, train acc:0.47, test acc:0.3734 ===\n",
            "train loss:1.4920007909124093\n",
            "train loss:1.6142972083231641\n",
            "train loss:1.6867743745348815\n",
            "=== epoch:245, train acc:0.49666666666666665, test acc:0.3776 ===\n",
            "train loss:1.6452295758532245\n",
            "train loss:1.4228518030826356\n",
            "train loss:1.6136053400036152\n",
            "=== epoch:246, train acc:0.49666666666666665, test acc:0.3801 ===\n",
            "train loss:1.5021570964217132\n",
            "train loss:1.4423789765225883\n",
            "train loss:1.5033567117595634\n",
            "=== epoch:247, train acc:0.49333333333333335, test acc:0.3792 ===\n",
            "train loss:1.530106613285267\n",
            "train loss:1.6477547711305067\n",
            "train loss:1.521778357068521\n",
            "=== epoch:248, train acc:0.5033333333333333, test acc:0.3839 ===\n",
            "train loss:1.4727983212088858\n",
            "train loss:1.4618607643758545\n",
            "train loss:1.4757188387838949\n",
            "=== epoch:249, train acc:0.51, test acc:0.387 ===\n",
            "train loss:1.5717935925411342\n",
            "train loss:1.5273493553722457\n",
            "train loss:1.4945072203072731\n",
            "=== epoch:250, train acc:0.5133333333333333, test acc:0.3886 ===\n",
            "train loss:1.5905173083064161\n",
            "train loss:1.5109035628041803\n",
            "train loss:1.3466697566069914\n",
            "=== epoch:251, train acc:0.51, test acc:0.3874 ===\n",
            "train loss:1.5520933440765305\n",
            "train loss:1.468508993123379\n",
            "train loss:1.47293936630649\n",
            "=== epoch:252, train acc:0.51, test acc:0.3886 ===\n",
            "train loss:1.4970784037947924\n",
            "train loss:1.494986393541848\n",
            "train loss:1.354380586292102\n",
            "=== epoch:253, train acc:0.51, test acc:0.3901 ===\n",
            "train loss:1.5092054930629883\n",
            "train loss:1.5451848573338587\n",
            "train loss:1.512985551989609\n",
            "=== epoch:254, train acc:0.52, test acc:0.3915 ===\n",
            "train loss:1.5132917965645458\n",
            "train loss:1.5453885056638235\n",
            "train loss:1.4400091739379786\n",
            "=== epoch:255, train acc:0.5233333333333333, test acc:0.396 ===\n",
            "train loss:1.4709246465161963\n",
            "train loss:1.4560967324301115\n",
            "train loss:1.4573711250291248\n",
            "=== epoch:256, train acc:0.5166666666666667, test acc:0.3967 ===\n",
            "train loss:1.4961441069448842\n",
            "train loss:1.5500026938299758\n",
            "train loss:1.355383024542176\n",
            "=== epoch:257, train acc:0.52, test acc:0.395 ===\n",
            "train loss:1.4616764575579766\n",
            "train loss:1.4809230804587716\n",
            "train loss:1.4822792750723364\n",
            "=== epoch:258, train acc:0.52, test acc:0.398 ===\n",
            "train loss:1.55944769402957\n",
            "train loss:1.5456490373982483\n",
            "train loss:1.4397490758703668\n",
            "=== epoch:259, train acc:0.5166666666666667, test acc:0.4035 ===\n",
            "train loss:1.4416190916379379\n",
            "train loss:1.4253379922235245\n",
            "train loss:1.3851080078112232\n",
            "=== epoch:260, train acc:0.5266666666666666, test acc:0.4097 ===\n",
            "train loss:1.4655581218161822\n",
            "train loss:1.349608948503678\n",
            "train loss:1.3948598254931883\n",
            "=== epoch:261, train acc:0.5266666666666666, test acc:0.412 ===\n",
            "train loss:1.5381169072302066\n",
            "train loss:1.3866906813837065\n",
            "train loss:1.471164283971625\n",
            "=== epoch:262, train acc:0.5366666666666666, test acc:0.4147 ===\n",
            "train loss:1.370066771592729\n",
            "train loss:1.4789862591206173\n",
            "train loss:1.4266914510917934\n",
            "=== epoch:263, train acc:0.54, test acc:0.4178 ===\n",
            "train loss:1.4190429574747327\n",
            "train loss:1.3395094773157308\n",
            "train loss:1.414507253016923\n",
            "=== epoch:264, train acc:0.54, test acc:0.4205 ===\n",
            "train loss:1.392172848306144\n",
            "train loss:1.3832233055312488\n",
            "train loss:1.4488383612806661\n",
            "=== epoch:265, train acc:0.54, test acc:0.4225 ===\n",
            "train loss:1.2309326584311424\n",
            "train loss:1.331104444206797\n",
            "train loss:1.4226840499021935\n",
            "=== epoch:266, train acc:0.5466666666666666, test acc:0.4236 ===\n",
            "train loss:1.467410730782016\n",
            "train loss:1.4120136343905094\n",
            "train loss:1.4535788831243779\n",
            "=== epoch:267, train acc:0.5466666666666666, test acc:0.4205 ===\n",
            "train loss:1.5148560924020933\n",
            "train loss:1.3927911990251414\n",
            "train loss:1.3677908530693421\n",
            "=== epoch:268, train acc:0.55, test acc:0.4279 ===\n",
            "train loss:1.4712530183751271\n",
            "train loss:1.4135874931783114\n",
            "train loss:1.2909353549100389\n",
            "=== epoch:269, train acc:0.55, test acc:0.4316 ===\n",
            "train loss:1.430226566730269\n",
            "train loss:1.3510826384700692\n",
            "train loss:1.3077768187338958\n",
            "=== epoch:270, train acc:0.56, test acc:0.4272 ===\n",
            "train loss:1.2533206921087694\n",
            "train loss:1.36823797752347\n",
            "train loss:1.3355315867083872\n",
            "=== epoch:271, train acc:0.56, test acc:0.4266 ===\n",
            "train loss:1.3664908622720962\n",
            "train loss:1.2653964569028695\n",
            "train loss:1.3416312401094554\n",
            "=== epoch:272, train acc:0.5533333333333333, test acc:0.4209 ===\n",
            "train loss:1.3625908357913794\n",
            "train loss:1.2688692002226587\n",
            "train loss:1.267349782885633\n",
            "=== epoch:273, train acc:0.5466666666666666, test acc:0.4193 ===\n",
            "train loss:1.3777667638941917\n",
            "train loss:1.3822418333379958\n",
            "train loss:1.440978399151508\n",
            "=== epoch:274, train acc:0.5533333333333333, test acc:0.4244 ===\n",
            "train loss:1.3133034812346742\n",
            "train loss:1.3440266308282893\n",
            "train loss:1.1499573101860576\n",
            "=== epoch:275, train acc:0.56, test acc:0.4276 ===\n",
            "train loss:1.3711366364056476\n",
            "train loss:1.2443377221640741\n",
            "train loss:1.3193428432351881\n",
            "=== epoch:276, train acc:0.5566666666666666, test acc:0.4279 ===\n",
            "train loss:1.3776365343137553\n",
            "train loss:1.3811717904712981\n",
            "train loss:1.29428240822257\n",
            "=== epoch:277, train acc:0.5566666666666666, test acc:0.431 ===\n",
            "train loss:1.3383550412236165\n",
            "train loss:1.3515438864340859\n",
            "train loss:1.244809424740056\n",
            "=== epoch:278, train acc:0.56, test acc:0.4281 ===\n",
            "train loss:1.294467070329495\n",
            "train loss:1.3809483760332382\n",
            "train loss:1.2643852559346274\n",
            "=== epoch:279, train acc:0.5666666666666667, test acc:0.428 ===\n",
            "train loss:1.29756998134936\n",
            "train loss:1.210610906822247\n",
            "train loss:1.2433214661965182\n",
            "=== epoch:280, train acc:0.56, test acc:0.43 ===\n",
            "train loss:1.1595159027133102\n",
            "train loss:1.239554429693997\n",
            "train loss:1.2364039433677152\n",
            "=== epoch:281, train acc:0.56, test acc:0.4344 ===\n",
            "train loss:1.2555358725566457\n",
            "train loss:1.1441289747834684\n",
            "train loss:1.315909314169535\n",
            "=== epoch:282, train acc:0.5633333333333334, test acc:0.4362 ===\n",
            "train loss:1.1710798206858388\n",
            "train loss:1.1977315423738875\n",
            "train loss:1.3905644013918095\n",
            "=== epoch:283, train acc:0.5633333333333334, test acc:0.4316 ===\n",
            "train loss:1.2929576049776141\n",
            "train loss:1.2461409808712325\n",
            "train loss:1.300090107897124\n",
            "=== epoch:284, train acc:0.5733333333333334, test acc:0.4319 ===\n",
            "train loss:1.344482747544735\n",
            "train loss:1.335211488000055\n",
            "train loss:1.3779356010347183\n",
            "=== epoch:285, train acc:0.5733333333333334, test acc:0.4365 ===\n",
            "train loss:1.316120156437084\n",
            "train loss:1.2304599422855935\n",
            "train loss:1.2693152386031687\n",
            "=== epoch:286, train acc:0.5766666666666667, test acc:0.44 ===\n",
            "train loss:1.2015118753528586\n",
            "train loss:1.288704868697594\n",
            "train loss:1.1789671386509644\n",
            "=== epoch:287, train acc:0.58, test acc:0.4444 ===\n",
            "train loss:1.191575740935907\n",
            "train loss:1.2123290707951646\n",
            "train loss:1.1560473329837508\n",
            "=== epoch:288, train acc:0.5766666666666667, test acc:0.4496 ===\n",
            "train loss:1.2623667298697783\n",
            "train loss:1.0557781708569516\n",
            "train loss:1.3707739657641285\n",
            "=== epoch:289, train acc:0.57, test acc:0.4485 ===\n",
            "train loss:1.3026037693387866\n",
            "train loss:1.1241339108147834\n",
            "train loss:1.2296501174005194\n",
            "=== epoch:290, train acc:0.5733333333333334, test acc:0.4481 ===\n",
            "train loss:1.2289007497141435\n",
            "train loss:1.1874150011807239\n",
            "train loss:1.3135204121235051\n",
            "=== epoch:291, train acc:0.58, test acc:0.4549 ===\n",
            "train loss:1.3308224677397862\n",
            "train loss:1.0682004114760923\n",
            "train loss:1.4042054352505615\n",
            "=== epoch:292, train acc:0.5766666666666667, test acc:0.4488 ===\n",
            "train loss:1.141642027334978\n",
            "train loss:1.2060375075195475\n",
            "train loss:1.2713697619278208\n",
            "=== epoch:293, train acc:0.5733333333333334, test acc:0.4479 ===\n",
            "train loss:1.3415135534940723\n",
            "train loss:1.2778805907880548\n",
            "train loss:1.3051479044905134\n",
            "=== epoch:294, train acc:0.5866666666666667, test acc:0.4555 ===\n",
            "train loss:1.2364608499729781\n",
            "train loss:1.1861796099351596\n",
            "train loss:1.2535682151383443\n",
            "=== epoch:295, train acc:0.5833333333333334, test acc:0.4562 ===\n",
            "train loss:1.3477004779190596\n",
            "train loss:1.1174587154206739\n",
            "train loss:1.2198993521219663\n",
            "=== epoch:296, train acc:0.5933333333333334, test acc:0.4578 ===\n",
            "train loss:1.3944978728566502\n",
            "train loss:1.1122900021564488\n",
            "train loss:1.0752241004469245\n",
            "=== epoch:297, train acc:0.5933333333333334, test acc:0.4625 ===\n",
            "train loss:1.1656870535384878\n",
            "train loss:1.097954817248133\n",
            "train loss:1.0841964329234182\n",
            "=== epoch:298, train acc:0.6, test acc:0.4666 ===\n",
            "train loss:1.2878271676319815\n",
            "train loss:1.1102101153351671\n",
            "train loss:1.3479061831545254\n",
            "=== epoch:299, train acc:0.6033333333333334, test acc:0.4649 ===\n",
            "train loss:1.0678436698100837\n",
            "train loss:1.1580986842173855\n",
            "train loss:1.1343814926385094\n",
            "=== epoch:300, train acc:0.5966666666666667, test acc:0.467 ===\n",
            "train loss:1.1044671688076912\n",
            "train loss:1.149056085102837\n",
            "train loss:1.2385972053809702\n",
            "=== epoch:301, train acc:0.5933333333333334, test acc:0.4691 ===\n",
            "train loss:1.2448109337855242\n",
            "train loss:1.2440119619153789\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.4698\n"
          ]
        }
      ],
      "source": [
        "# 1. mnist  \n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "#       \n",
        "x_train = x_train[:300]\n",
        "t_train = t_train[:300]\n",
        "\n",
        "#   \n",
        "use_dropout = True\n",
        "dropout_ratio = 0.2\n",
        "\n",
        "# 2.  \n",
        "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
        "                        output_size=10, use_dropout = use_dropout, dropout_ration=dropout_ratio)\n",
        "\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test, \n",
        "                  epochs=301,\n",
        "                  mini_batch_size=100,\n",
        "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
        "trainer.train()\n",
        "\n",
        "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 4. \n",
        "markers = {'train' : 'o', 'test': 's'}\n",
        "x = np.arange(len(train_acc_list))\n",
        "plt.plot(x, train_acc_list, marker = 'o', label='train', markevery = 10)\n",
        "plt.plot(x, test_acc_list, marker = 's', label='test', markevery = 10)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
